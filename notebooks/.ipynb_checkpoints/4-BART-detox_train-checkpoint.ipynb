{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICE'] = '0'\n",
    "\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_name = 'facebook/bart-base'\n",
    "model_name = 'SkolkovoInstitute/bart-base-detox'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../data/external/paradetox-data/'\n",
    "dataset = datasets.load_dataset(dataset_path)['train']\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07828777c78549c8b94ae608abc3bd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "detoxified_ressults = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    toxic_batch, neutral_batch = batch['en_toxic_comment'], batch['en_neutral_comment']\n",
    "    tokenized_toxic_batch = tokenizer(toxic_batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "    responce = model.generate(**tokenized_toxic_batch, max_new_tokens=512)\n",
    "    results = tokenizer.batch_decode(responce, skip_special_tokens=True)\n",
    "    detoxified_ressults.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19744,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "detoxified_ressults = np.array(detoxified_ressults).flatten()\n",
    "detoxified_ressults.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(\"BART_paraphrased\", detoxified_ressults)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch.nn as nn\n",
    "# tokenizer and model weights for calculating toxisity score of the text\n",
    "toxic_tokenizer = RobertaTokenizer.from_pretrained(\n",
    "    \"SkolkovoInstitute/roberta_toxicity_classifier\"\n",
    ")\n",
    "toxic_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"SkolkovoInstitute/roberta_toxicity_classifier\", device_map='auto'\n",
    ")\n",
    "\n",
    "def get_toxisity_score(model_output):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    scores = sigmoid(model_output.squeeze()).cpu().detach().numpy()\n",
    "    result = {\"neutral\": scores[0], \"toxic\": scores[1]}\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_classification(\n",
    "    text, classification_tokenizer=toxic_tokenizer, classification_model=toxic_model\n",
    "):\n",
    "    tokenized_text = classification_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    predictions = classification_model(tokenized_text)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c2cd8b5d654e13a2bd4d7dff89eb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_toxicity_val = []\n",
    "detoxified_toxicity_val = []\n",
    "masked_toxicity_val = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    toxic_comments_batch, detoxified_comments_batch, masked_comments_batch = (\n",
    "        batch[\"en_toxic_comment\"],\n",
    "        batch[\"en_neutral_comment\"],\n",
    "        batch[\"BART_paraphrased\"],\n",
    "    )\n",
    "    t = toxic_tokenizer(toxic_comments_batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "    res = toxic_model(**t)\n",
    "    for elem in res.logits:\n",
    "        init_toxicity_val.append(get_toxisity_score(elem)['toxic'])\n",
    "\n",
    "    t = toxic_tokenizer(detoxified_comments_batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "    res = toxic_model(**t)\n",
    "    for elem in res.logits:\n",
    "        detoxified_toxicity_val.append(get_toxisity_score(elem)['toxic'])\n",
    "\n",
    "    t = toxic_tokenizer(masked_comments_batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "    res = toxic_model(**t)\n",
    "    for elem in res.logits:\n",
    "        masked_toxicity_val.append(get_toxisity_score(elem)['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.add_column('initial_toxicity', init_toxicity_val)\n",
    "dataset = dataset.add_column('ideal_toxicity', detoxified_toxicity_val)\n",
    "dataset = dataset.add_column('resulting_toxicity', masked_toxicity_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn as nn\n",
    "\n",
    "simmilarity_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "cosine_simmilarity = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "def get_sim(text1, text2):\n",
    "    embeddings = simmilarity_model.encode([text1, text2], convert_to_tensor=True)\n",
    "    return cosine_simmilarity(embeddings[0], embeddings[1]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78cc2ec50944433ab6cdd55739753ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reference2masked_sim = []\n",
    "reference2translation_sim = []\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    toxic = example['en_toxic_comment']\n",
    "    nontoxic = example['en_neutral_comment']\n",
    "    masked = example['BART_paraphrased']\n",
    "    reference2masked_sim.append(get_sim(toxic, masked))\n",
    "    reference2translation_sim.append(get_sim(toxic, nontoxic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.add_column('reference2masked_sim', reference2masked_sim)\n",
    "dataset = dataset.add_column('reference2translation_sim', reference2translation_sim)\n",
    "pandas_dataset_results = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure fluency of thee generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72173b9d876747d5853ea89a8ecffe60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79276cb99e446da8298cb20930be051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/628 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2557c4b35cd449d8b331e34ff1d526bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd901f7350df47208fedb723bff2f39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c7edf152ce401aa06c8d8dad324dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fb49913b8e4e04917035f8976e438d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faba05ebdc034961b5d4774f4b63256a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "fluency_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/roberta-large-cola-krishna2020\")\n",
    "fluency_model = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/roberta-large-cola-krishna2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fluent': 0.37349015, 'Non-Fluent': 0.68185806}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_fluency_score(logits):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    scores = sigmoid(logits.squeeze()).cpu().detach().numpy()\n",
    "    result = {\"Fluent\": scores[0], \"Non-Fluent\": scores[1]}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb9ee2741924591ab99695b8057b574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fluency_scores = []\n",
    "for example in tqdm(dataset):\n",
    "    text = example['BART_paraphrased']\n",
    "    tokens = fluency_tokenizer(text, return_tensors='pt')\n",
    "    fluency_scores.append(get_fluency_score(fluency_model(**tokens).logits)['Fluent'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commit-generation",
   "language": "python",
   "name": "commit-generation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
