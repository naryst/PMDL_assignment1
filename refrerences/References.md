## Models:

1) BART for text detoxification - https://huggingface.co/s-nlp/bart-base-detox
2) T5 for text paraphrasing - https://huggingface.co/ceshine/t5-paraphrase-paws-msrp-opinosis
3) Fluency computing - https://huggingface.co/cointegrated/roberta-large-cola-krishna2020
4) Toxicity classifier - https://huggingface.co/s-nlp/roberta_toxicity_classifier
5) Text simmilarity count - https://huggingface.co/sentence-transformers/all-mpnet-base-v2

## Datasets:

1) Paradetox dataset, used for testing - https://huggingface.co/datasets/s-nlp/paradetox

2) Toxic comments dataset (*Not used because of ti complex structure of the toxic comments*) - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge

## Code references:

1) https://github.com/dardem/text_detoxification
2) https://dardem.github.io/text_detoxification/
3) https://github.com/s-nlp/paradetox
4) https://github.com/s-nlp/detox
5) https://colab.research.google.com/drive/1xTqbx7IPF8bVL2bDCfQSDarA43mIPefE?usp=sharing#scrollTo=NmxgPN3Z82Bp (Code to count metrics of text detoxification)

